---
layout: default
title: Ceph的架构
author: lijiaocn
createdate: 2017/05/27 11:33:01
changedate: 2017/06/02 17:56:42
categories: 项目
tags: ceph 存储
keywords: ceph,架构
description: ceph是一个可同时提供对象存储、块设备、网络文件系统的分布式存储系统。

---

* auto-gen TOC:
{:toc}

## arch 

Ceph系统由三部分组成：

	Ceph OSDs:  负责数据存储、备份、恢复。
	Mointors:   维护集群状态图(cluster map)。
	MDSs:       保存元数据，只有Ceph Filesystem需要。

Ceph底层是RADOS，在RADOS的基础上开发了以下组件:

	LIBRADOS:    可以直接操作RADOS的library，支持C/C++/Java/Python/Ruby/PHP。
	RADOSGW:     对象存储网关，兼容S3和Swift。
	RBD:         块存储，支持linux kernel client、QEMU/KVM driver。
	CEPH FS:     文件系统，兼容POSIX，支持linux kernel client、FUSE。

无论是从对象存储、块存储和文件存储，写入的数据到达RADOS时，都被作为一个object文件存放在指定的OSD的文件系统中。

[RADOS][3]是一个可靠、自治、分布式的对象存储系统，由OSD和Monitor组成。

OSD和Client通过[CRUSH][4]算法计算出数据的存储位置后，直接与目标OSD通信，从而实现了去中心化。

OSD和Client都需要知晓集群的状态图(cluster map)，cluster map由5个map组成:

	Monitor map:   每个monitor的状态和当前的epoch，`ceph mon dump`
	OSD map:       pools、备份数、PG number、OSD列表, `ceph osd dump`
	PG map:        
	CRUSH map:
	MDS map:       `ceph fs dump`

cluster map由monitor维护，client读写数据之前先从monitor中获取当前的cluster map。

多个monitor之间通过[Paxos][5]协议达成一致意见。

从client的视角来看，每个object存放在指定的pool，object和pool都是命名的，每个pool中会由一个设置好的PG数目。

每个block通过下面的方式得到一个PG ID，PG ID则映射到具体的OSD。

	1. The client inputs the pool ID and the object ID. (e.g., pool = “liverpool” and object-id = “john”)
	2. Ceph takes the object ID and hashes it.
	3. Ceph calculates the hash modulo the number of PGs. (e.g., 58) to get a PG ID.
	4. Ceph gets the pool ID given the pool name (e.g., “liverpool” = 4)
	5. Ceph prepends the pool ID to the PG ID (e.g., 4.58).

注意PG ID由`目标pool的ID`和`Object ID对应到的PG ID`组成。

## 参考

1. [ceph docs][1]
2. [ceph architecture][2]
3. [RADOS: A Scalable, Reliable Storage Service for Petabyte-scale Storage Clusters][3]
4. [CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data][4]
5. [Paxos][5]

[1]: http://docs.ceph.com/docs/master/  "ceph docs" 
[2]: http://docs.ceph.com/docs/master/architecture/ "ceph architecture"
[3]: https://ceph.com/wp-content/uploads/2016/08/weil-rados-pdsw07.pdf "RADOS: A Scalable, Reliable Storage Service for Petabyte-scale Storage Clusters"
[4]: https://ceph.com/wp-content/uploads/2016/08/weil-crush-sc06.pdf "CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data"
[5]: https://en.wikipedia.org/wiki/Paxos_(computer_science) "Paxos"
